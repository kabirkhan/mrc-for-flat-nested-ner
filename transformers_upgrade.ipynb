{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocess.generate_mrc_dataset import generate_query_ner_dataset\n",
    "from data_preprocess.file_utils import load_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_path = \"../wlp_wnut/WNUT_2020_NER/data/test_data/Conll_Format/protocol_103_conll.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_query_ner_dataset(\"../wlp_wnut/data/ner/wlp_conll/train.txt\", \"../wlp_wnut/data/ner/wlp_mrc_query/train_mrc_query.json\", \"flat\", \"en_wnut_20_wlp\")\n",
    "# generate_query_ner_dataset(\"../wlp_wnut/data/ner/wlp_conll/dev.txt\", \"../wlp_wnut/data/ner/wlp_mrc_query/dev_mrc_query.json\", \"flat\", \"en_wnut_20_wlp\")\n",
    "# generate_query_ner_dataset(\"../wlp_wnut/data/ner/wlp_conll/test.txt\", \"../wlp_wnut/data/ner/wlp_mrc_query/test_mrc_query.json\", \"flat\", \"en_wnut_20_wlp\")\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../wlp_wnut/data/ner/wlp_mrc_query_test\", exist_ok=True)\n",
    "\n",
    "generate_query_ner_dataset(\"../wlp_wnut/WNUT_2020_NER/data/train_data/Conll_Format/protocol_100_conll.txt\", \"../wlp_wnut/data/ner/wlp_mrc_query_test/mrc-ner.train\", \"flat\", \"en_wnut_20_wlp\")\n",
    "generate_query_ner_dataset(\"../wlp_wnut/WNUT_2020_NER/data/dev_data/Conll_Format/protocol_102_conll.txt\", \"../wlp_wnut/data/ner/wlp_mrc_query_test/mrc-ner.dev\", \"flat\", \"en_wnut_20_wlp\")\n",
    "generate_query_ner_dataset(\"../wlp_wnut/WNUT_2020_NER/data/test_data/Conll_Format/protocol_103_conll.txt\", \"../wlp_wnut/data/ner/wlp_mrc_query_test/mrc-ner.test\", \"flat\", \"en_wnut_20_wlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import argparse \n",
    "import numpy as np \n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from data_loader.model_config import Config \n",
    "from data_loader.mrc_data_loader import MRCNERDataLoader\n",
    "from data_loader.mrc_data_processor import Conll03Processor, MSRAProcessor, Onto4ZhProcessor, Onto5EngProcessor, GeniaProcessor, ACE2004Processor, ACE2005Processor, ResumeZhProcessor, QueryNERProcessor\n",
    "from data_loader.mrc_utils import convert_examples_to_features\n",
    "from layer.optim import AdamW, lr_linear_decay, BertAdam\n",
    "from model.bert_mrc import BertQueryNER\n",
    "from data_loader.bert_tokenizer import BertTokenizer4Tagger \n",
    "from metric.mrc_ner_evaluate  import flat_ner_performance, nested_ner_performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export REPO_PATH=.\n",
    "export PYTHONPATH=\"$PYTHONPATH:$REPO_PATH\"\n",
    "export CONFIG_PATH=${FOLDER_PATH}/config/en_bert_base_cased.json\n",
    "export DATA_PATH=~/Documents/wlp_wnut/data/ner/wlp_mrc_query\n",
    "export BERT_PATH=bert-base-cased\n",
    "export OUTPUT_PATH=./wlp_wnut_output\n",
    "export DATA_SIGN=en_wnut_20_wlp \n",
    "export ENTITY_SIGN=flat\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 python3 $REPO_PATH/run/train_bert_mrc.py \\\n",
    "--data_dir $DATA_PATH \\\n",
    "--n_gpu $N_GPU \\\n",
    "--entity_sign $ENTITY_SIGN \\\n",
    "--data_sign $DATA_SIGN \\\n",
    "--bert_model $BERT_PATH \\\n",
    "--config_path $CONFIG_PATH \\\n",
    "--output_dir $OUTPUT_PATH \\\n",
    "--train_batch_size 4 \\\n",
    "--dev_batch_size 4 \\\n",
    "--test_batch_size 4 \\\n",
    "--num_train_epochs 5 \\\n",
    "--n_gpu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WlpWnut20Processor(QueryNERProcessor):\n",
    "    def get_labels(self, ):\n",
    "        return [\n",
    "            \"Action\",\n",
    "            \"Reagent\",\n",
    "            \"Modifier\",\n",
    "            \"Location\",\n",
    "            \"Amount\",\n",
    "            \"Time\",\n",
    "            \"Method\",\n",
    "            \"Concentration\",\n",
    "            \"Temperature\",\n",
    "            \"Device\",\n",
    "            \"Measure-Type\",\n",
    "            \"Numerical\",\n",
    "            \"Speed\",\n",
    "            \"Generic-Measure\",\n",
    "            \"Size\",\n",
    "            \"Seal\",\n",
    "            \"pH\",\n",
    "            \"Mention\", \n",
    "            \"O\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "\n",
    "    print(\"-*-\"*10)\n",
    "    print(\"current data_sign: {}\".format(config.data_sign))\n",
    "\n",
    "    if config.data_sign == \"conll03\":\n",
    "        data_processor = Conll03Processor()\n",
    "    elif config.data_sign == \"zh_msra\":\n",
    "        data_processor = MSRAProcessor()\n",
    "    elif config.data_sign == \"zh_onto\":\n",
    "        data_processor = Onto4ZhProcessor()\n",
    "    elif config.data_sign == \"en_onto\":\n",
    "        data_processor = Onto5EngProcessor()\n",
    "    elif config.data_sign == \"genia\":\n",
    "        data_processor = GeniaProcessor()\n",
    "    elif config.data_sign == \"ace2004\":\n",
    "        data_processor = ACE2004Processor()\n",
    "    elif config.data_sign == \"ace2005\":\n",
    "        data_processor = ACE2005Processor()\n",
    "    elif config.data_sign == \"resume\":\n",
    "        data_processor = ResumeZhProcessor()\n",
    "    elif config.data_sign == \"en_wnut_20_wlp\":\n",
    "        data_processor = WlpWnut20Processor()\n",
    "    else:\n",
    "        raise ValueError(\"Please Notice that your data_sign DO NOT exits !!!!!\")\n",
    "\n",
    "\n",
    "    label_list = data_processor.get_labels()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.bert_model)\n",
    "    \n",
    "    print(tokenizer)\n",
    "\n",
    "    dataset_loaders = MRCNERDataLoader(config, data_processor, label_list, tokenizer, mode=\"train\", allow_impossible=True)\n",
    "    train_dataloader = dataset_loaders.get_dataloader(data_sign=\"train\", num_data_processor=config.num_data_processor)\n",
    "    dev_dataloader = dataset_loaders.get_dataloader(data_sign=\"dev\", num_data_processor=config.num_data_processor)\n",
    "    test_dataloader = dataset_loaders.get_dataloader(data_sign=\"test\", num_data_processor=config.num_data_processor)\n",
    "    num_train_steps = dataset_loaders.get_num_train_epochs()\n",
    "\n",
    "    return train_dataloader, dev_dataloader, test_dataloader, num_train_steps, label_list \n",
    "\n",
    "\n",
    "def load_model(config, num_train_steps, label_list):\n",
    "    device = torch.device(\"cuda\") \n",
    "    n_gpu = config.n_gpu\n",
    "    model = BertQueryNER(config, ) \n",
    "    model.to(device)\n",
    "    if config.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # prepare optimzier \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\", 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "    {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.01)\n",
    "    # optimizer = BertAdam(optimizer_grouped_parameters, lr=config.learning_rate, warmup=config.warmup_proportion,\n",
    "    #                     t_total=num_train_steps, max_grad_norm=config.clip_grad)\n",
    "    sheduler = None\n",
    "\n",
    "    if config.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=config.amp_level)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if config.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[config.local_rank], output_device=config.local_rank, find_unused_parameters=True\n",
    "            )\n",
    "\n",
    "    return model, optimizer, sheduler, device, n_gpu\n",
    "\n",
    "\n",
    "def train(model, optimizer, sheduler,  train_dataloader, dev_dataloader, test_dataloader, config, \\\n",
    "    device, n_gpu, label_list):\n",
    "\n",
    "    dev_best_acc = 0 \n",
    "    dev_best_precision = 0 \n",
    "    dev_best_recall = 0 \n",
    "    dev_best_f1 = 0 \n",
    "    dev_best_loss = 10000000000000\n",
    "\n",
    "    test_acc_when_dev_best = 0 \n",
    "    test_pre_when_dev_best = 0 \n",
    "    test_rec_when_dev_best = 0 \n",
    "    test_f1_when_dev_best = 0 \n",
    "    test_loss_when_dev_best = 1000000000000000\n",
    "\n",
    "    model.train()\n",
    "    for idx in range(int(config.num_train_epochs)):\n",
    "        tr_loss = 0 \n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        print(\"#######\"*10)\n",
    "        print(\"EPOCH: \", str(idx))\n",
    "        if idx != 0:\n",
    "            lr_linear_decay(optimizer) \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch) \n",
    "            input_ids, input_mask, segment_ids, start_pos, end_pos, span_pos, span_label_mask, ner_cate = batch\n",
    "\n",
    "            loss = model(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, \\\n",
    "                start_positions=start_pos, end_positions=end_pos, span_positions=span_pos, span_label_mask=span_label_mask)\n",
    "\n",
    "            if config.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "\n",
    "            if config.fp16:\n",
    "                from apex import amp\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), config.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1 \n",
    "\n",
    "            if nb_tr_steps % config.checkpoint == 0:\n",
    "                print(\"-*-\"*15)\n",
    "                print(\"current training loss is : \")\n",
    "                print(loss.item())\n",
    "                model, tmp_dev_loss, tmp_dev_acc, tmp_dev_prec, tmp_dev_rec, tmp_dev_f1 = eval_checkpoint(model, dev_dataloader, config, device, n_gpu, label_list, eval_sign=\"dev\")\n",
    "                print(\"......\"*10)\n",
    "                print(\"DEV: loss, acc, precision, recall, f1\")\n",
    "                print(tmp_dev_loss, tmp_dev_acc, tmp_dev_prec, tmp_dev_rec, tmp_dev_f1)\n",
    "\n",
    "                if tmp_dev_f1 > dev_best_f1 :\n",
    "                    dev_best_acc = tmp_dev_acc \n",
    "                    dev_best_loss = tmp_dev_loss \n",
    "                    dev_best_precision = tmp_dev_prec \n",
    "                    dev_best_recall = tmp_dev_rec \n",
    "                    dev_best_f1 = tmp_dev_f1 \n",
    "\n",
    "                    # export model \n",
    "                    if config.export_model:\n",
    "                        model_to_save = model.module if hasattr(model, \"module\") else model \n",
    "                        output_model_file = os.path.join(config.output_dir, \"bert_finetune_model_{}_{}.bin\".format(str(idx),str(nb_tr_steps)))\n",
    "                        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                        print(\"SAVED model path is :\") \n",
    "                        print(output_model_file)\n",
    "\n",
    "                    model = model.cuda().to(device)\n",
    "                    model, tmp_test_loss, tmp_test_acc, tmp_test_prec, tmp_test_rec, tmp_test_f1 = eval_checkpoint(model, test_dataloader, config, device, n_gpu, label_list, eval_sign=\"test\")\n",
    "                    print(\"......\"*10)\n",
    "                    print(\"TEST: loss, acc, precision, recall, f1\")\n",
    "                    print(tmp_test_loss, tmp_test_acc, tmp_test_prec, tmp_test_rec, tmp_test_f1)\n",
    "\n",
    "                    test_acc_when_dev_best = tmp_test_acc \n",
    "                    test_pre_when_dev_best = tmp_test_prec\n",
    "                    test_rec_when_dev_best = tmp_test_rec\n",
    "                    test_f1_when_dev_best = tmp_test_f1 \n",
    "                    test_loss_when_dev_best = tmp_test_loss\n",
    "                    model = model.cuda().to(device)\n",
    "\n",
    "                print(\"-*-\"*15)\n",
    "\n",
    "    print(\"=&=\"*15)\n",
    "    print(\"Best DEV : overall best loss, acc, precision, recall, f1 \")\n",
    "    print(dev_best_loss, dev_best_acc, dev_best_precision, dev_best_recall, dev_best_f1)\n",
    "    print(\"scores on TEST when Best DEV:loss, acc, precision, recall, f1 \")\n",
    "    print(test_loss_when_dev_best, test_acc_when_dev_best, test_pre_when_dev_best, test_rec_when_dev_best, test_f1_when_dev_best)\n",
    "    print(\"=&=\"*15)\n",
    "\n",
    "\n",
    "def eval_checkpoint(model_object, eval_dataloader, config, device, n_gpu, label_list, eval_sign=\"dev\"):\n",
    "    # input_dataloader type can only be one of dev_dataloader, test_dataloader\n",
    "\n",
    "    eval_loss = 0\n",
    "    start_pred_lst = []\n",
    "    end_pred_lst = []\n",
    "    span_pred_lst = []\n",
    "    start_scores_lst = []\n",
    "    end_scores_lst = []\n",
    "    mask_lst = []\n",
    "    start_gold_lst = []\n",
    "    span_gold_lst = []\n",
    "    end_gold_lst = []\n",
    "    eval_steps = 0\n",
    "    ner_cate_lst = []\n",
    "\n",
    "    for input_ids, input_mask, segment_ids, start_pos, end_pos, span_pos, span_label_mask, ner_cate in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        start_pos = start_pos.to(device)\n",
    "        end_pos = end_pos.to(device)\n",
    "        span_pos = span_pos.to(device)\n",
    "        span_label_mask = span_label_mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_object.eval()\n",
    "            tmp_eval_loss = model_object(input_ids, segment_ids, input_mask, start_pos, end_pos, span_pos, span_label_mask)\n",
    "            start_labels, end_labels, span_scores = model_object(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        start_pos = start_pos.to(\"cpu\").numpy().tolist()\n",
    "        end_pos = end_pos.to(\"cpu\").numpy().tolist()\n",
    "        span_pos = span_pos.to(\"cpu\").numpy().tolist()\n",
    "\n",
    "        start_label = start_labels.detach().cpu().numpy().tolist()\n",
    "        end_label = end_labels.detach().cpu().numpy().tolist()\n",
    "        span_scores = span_scores.detach().cpu().numpy().tolist()\n",
    "        span_label = span_scores\n",
    "        input_mask = input_mask.to(\"cpu\").detach().numpy().tolist()\n",
    "\n",
    "        ner_cate_lst += ner_cate.numpy().tolist()\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        mask_lst += input_mask \n",
    "        eval_steps += 1\n",
    "\n",
    "        start_pred_lst += start_label \n",
    "        end_pred_lst += end_label \n",
    "        span_pred_lst += span_label\n",
    "        \n",
    "        start_gold_lst += start_pos \n",
    "        end_gold_lst += end_pos \n",
    "        span_gold_lst += span_pos \n",
    "\n",
    "    \n",
    "    if config.entity_sign == \"flat\":\n",
    "        eval_accuracy, eval_precision, eval_recall, eval_f1 = flat_ner_performance(start_pred_lst, end_pred_lst, span_pred_lst, start_gold_lst, end_gold_lst, span_gold_lst, ner_cate_lst, label_list, threshold=config.entity_threshold, dims=2)\n",
    "    else:\n",
    "        eval_accuracy, eval_precision, eval_recall, eval_f1 = nested_ner_performance(start_pred_lst, end_pred_lst, span_pred_lst, start_gold_lst, end_gold_lst, span_gold_lst, ner_cate_lst, label_list, threshold=config.entity_threshold, dims=2)\n",
    "\n",
    "    average_loss = round(eval_loss / eval_steps, 4)  \n",
    "    eval_f1 = round(eval_f1 , 4)\n",
    "    eval_precision = round(eval_precision , 4)\n",
    "    eval_recall = round(eval_recall , 4) \n",
    "    eval_accuracy = round(eval_accuracy , 4)\n",
    "    model_object.train()\n",
    "\n",
    "    return model_object, average_loss, eval_accuracy, eval_precision, eval_recall, eval_f1\n",
    "\n",
    "\n",
    "def merge_config(args_config):\n",
    "    model_config_path = args_config.config_path \n",
    "    model_config = Config.from_json_file(model_config_path)\n",
    "    model_config.update_args(args_config)\n",
    "    model_config.print_config()\n",
    "    return model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "\n",
    "args.config_path = \"./config/en_bert_base_cased.json\"\n",
    "args.data_dir = \"../wlp_wnut/data/ner/wlp_mrc_query/\"\n",
    "args.bert_model = \"bert-base-cased\"\n",
    "args.task_name = None\n",
    "args.max_seq_length = 128\n",
    "args.train_batch_size = 4\n",
    "args.dev_batch_size = 4\n",
    "args.test_batch_size = 4\n",
    "args.checkpoint = 100\n",
    "args.learning_rate = 5e-5\n",
    "args.num_train_epochs = 5\n",
    "args.warmup_proportion = 0.1\n",
    "args.max_grad_norm = 1.0\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.seed = 3006\n",
    "args.output_dir = \"./wlp_wnut_output\"\n",
    "args.data_sign = \"en_wnut_20_wlp\"\n",
    "args.weight_start = 1.0\n",
    "args.weight_end = 1.0\n",
    "args.weight_span = 1.0\n",
    "args.entity_sign = \"flat\"\n",
    "args.n_gpu = 1\n",
    "args.dropout = 0.2\n",
    "args.entity_threshold = 0.5\n",
    "args.num_data_processor = 1\n",
    "args.data_cache = True\n",
    "args.export_model = True\n",
    "args.do_lower_case = False\n",
    "args.fp16 = False\n",
    "args.amp_level = \"02\" \n",
    "args.local_rank = -1\n",
    "args.num_data_processor = 1\n",
    "\n",
    "\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps \n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please notice that merge the args_dict and json_config ... ...\n",
      "{\n",
      "  \"bert_frozen\": \"false\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"classifier_sign\": \"multi_nonlinear\",\n",
      "  \"clip_grad\": 1,\n",
      "  \"bert_config\": {\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"vocab_size\": 28996\n",
      "  },\n",
      "  \"config_path\": \"./config/en_bert_base_cased.json\",\n",
      "  \"data_dir\": \"../wlp_wnut/data/ner/wlp_mrc_query/\",\n",
      "  \"bert_model\": \"bert-base-cased\",\n",
      "  \"task_name\": null,\n",
      "  \"max_seq_length\": 128,\n",
      "  \"train_batch_size\": 4,\n",
      "  \"dev_batch_size\": 4,\n",
      "  \"test_batch_size\": 4,\n",
      "  \"checkpoint\": 100,\n",
      "  \"learning_rate\": 5e-05,\n",
      "  \"num_train_epochs\": 5,\n",
      "  \"warmup_proportion\": 0.1,\n",
      "  \"max_grad_norm\": 1.0,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"seed\": 3006,\n",
      "  \"output_dir\": \"./wlp_wnut_output\",\n",
      "  \"data_sign\": \"en_wnut_20_wlp\",\n",
      "  \"weight_start\": 1.0,\n",
      "  \"weight_end\": 1.0,\n",
      "  \"weight_span\": 1.0,\n",
      "  \"entity_sign\": \"flat\",\n",
      "  \"n_gpu\": 1,\n",
      "  \"dropout\": 0.2,\n",
      "  \"entity_threshold\": 0.5,\n",
      "  \"num_data_processor\": 1,\n",
      "  \"data_cache\": true,\n",
      "  \"export_model\": true,\n",
      "  \"do_lower_case\": false,\n",
      "  \"fp16\": false,\n",
      "  \"amp_level\": \"02\",\n",
      "  \"local_rank\": -1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "args_config = args\n",
    "\n",
    "config = merge_config(args_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.zeros((100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*--*--*--*--*--*--*--*--*--*-\n",
      "current data_sign: en_wnut_20_wlp\n",
      "<transformers.tokenization_bert.BertTokenizer object at 0x7f4b8049e290>\n",
      "=*==*==*==*==*==*==*==*==*==*=\n",
      "loading train data ... ...\n",
      "Before Input Data: 145332\n",
      "After Input Data slice: 27000\n",
      "FEATURES SIZE 128\n",
      "EXAMPLES LENGTH 27000\n",
      "27000 train data loaded\n",
      "=*==*==*==*==*==*==*==*==*==*=\n",
      "loading dev data ... ...\n",
      "FEATURES SIZE 128\n",
      "EXAMPLES LENGTH 9000\n",
      "9000 dev data loaded\n",
      "=*==*==*==*==*==*==*==*==*==*=\n",
      "loading test data ... ...\n",
      "FEATURES SIZE 128\n",
      "EXAMPLES LENGTH 9000\n",
      "9000 test data loaded\n"
     ]
    }
   ],
   "source": [
    "train_loader, dev_loader, test_loader, num_train_steps, label_list = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27000, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.tensors[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, sheduler, device, n_gpu = load_model(config, num_train_steps, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "EPOCH:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.48953989148139954\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1515 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.021484261378645897\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1381 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.27062806487083435\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1451 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.019999917596578598\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1395 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.3444683849811554\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1368 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.30863073468208313\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1326 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.08610846102237701\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1314 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.03002155013382435\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1445 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.03611789643764496\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1511 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.9368778467178345\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.154 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.004764572251588106\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.166 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.020386312156915665\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1407 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.014621000736951828\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1472 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.15284310281276703\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.146 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.1286657303571701\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1473 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.2108306884765625\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1746 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.11198946833610535\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1333 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.014859002083539963\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1351 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.6448466181755066\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1394 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.12929192185401917\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1854 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.0255767572671175\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1575 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.12797358632087708\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.16 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.21934223175048828\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1654 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.3481614887714386\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1623 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.09974644333124161\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1578 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.020376676693558693\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.161 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.02371222712099552\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1595 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.6378493905067444\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1581 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.12253537029027939\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1577 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.0898757353425026\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1573 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.022668758407235146\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1589 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.4618905484676361\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1585 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.1700349599123001\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1579 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.0152017492800951\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.1652 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.024276701733469963\n",
      "............................................................\n",
      "DEV: loss, acc, precision, recall, f1\n",
      "0.159 0.791 0.0 0.0 0.0\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "-*--*--*--*--*--*--*--*--*--*--*--*--*--*--*-\n",
      "current training loss is : \n",
      "0.02488437108695507\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, sheduler, train_loader, dev_loader, test_loader, config, device, n_gpu, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
